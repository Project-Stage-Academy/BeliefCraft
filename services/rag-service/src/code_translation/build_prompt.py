translated_prompt = '''
There is a book that contains Julia code. This code needs to be translated into Python. Next to the code, there is also descriptive text explaining it. The code itself has already been translated — it just needs to be inserted correctly.

Your task is to update the description and replace the Julia code with the corresponding Python code. The description should be updated as if it had originally been written for the Python code, without any mention that something was translated or previously written in Julia.

Only update the parts of the description that specifically relate to the Julia code; everything else must remain unchanged. I will provide descriptions taken out of context, so they may reference things or variables that are not shown — do not modify those.

All variable names must be preserved, but some variables may be written using Greek letters — these should be transliterated into English.

Note that there may also be small formulas that look like code; do not modify them.

Here are the original fragments with Julia code and description:


{}


Here is the Python translation of those fragments — use it:

{}

This imported code is just for your understanding — these imported functions should NOT be added to the code, because they already exist elsewhere in the book.

Return the response as a JSON object as follows:
[
    {{
        "algorithm_number": "Algorithm X.X."
        "description": "...",
        "code": "...",
        "declarations": {{
            "julia_declaration": "corresponding_python_declaration",
        }}
    }},
    ...
]

In the declarations field, include any function or struct that are present in the original Julia code and their corresponding Python translations.
Include there only top-level declarations, i.e. function and struct definitions, but not variable assignments or type annotations.
The keys should be the original Julia declarations, and the values should be the corresponding Python declarations.
If it is a method definition, include the class name as well "ClassName.method_name".
'''




translate_prompt = '''
There is a book that contains Julia code. This code needs to be translated into Python. Next to the code, there is also descriptive text explaining it.

Your task is to update the description and replace the Julia code with the corresponding Python code. The description should be updated as if it had originally been written for the Python code, without any mention that something was translated or previously written in Julia.

Only update the parts of the description that specifically relate to the Julia code; everything else must remain unchanged. I will provide descriptions taken out of context, so they may reference things or variables that are not shown — do not modify those.

All variable names must be preserved, but some variables may be written using Greek letters — these should be transliterated into English.

Note that there may also be small formulas that look like code; do not modify them.

Return the response as a JSON object as follows:
[
    {{
        "algorithm_number": "Algorithm X.X."
        "description": "...",
        "code": "...",
        "declarations": {{
            "julia_declaration": "corresponding_python_declaration",
        }}
    }},
    ...
]

In the declarations field, include any function or struct that are present in the original Julia code and their corresponding Python translations.
Include there only top-level declarations, i.e. function and struct definitions, but not variable assignments or type annotations.
The keys should be the original Julia declarations, and the values should be the corresponding Python declarations.
If it is a method definition, include the class name as well "ClassName.method_name".


Here is an example of such translation. The original julia fragments:


```
Algorithm 5.1. An algorithm for computing the Bayesian score for a list of variables vars and a graph G given data D. This method uses a uniform prior  $ \\alpha_{ijk} = 1 $ for all i, j, and k as generated by algorithm 4.2. The loggamma function is provided by SpecialFunctions.jl. Chapter 4 introduced the statistics and prior functions. Note that  $ \\log(\\Gamma(\\alpha)/\\Gamma(\\alpha+m)) = \\log\\Gamma(\\alpha) - \\log\\Gamma(\\alpha+m) $, and that  $ \\log\\Gamma(1) = 0 $.</div>\n

function bayesian_score_component(M, α)
p = sum(loggamma.(α + M))
p -= sum(loggamma.(α))
p += sum(loggamma.(sum(α,dims=2)))
p -= sum(loggamma.(sum(α,dims=2) + sum(M,dims=2)))
return p
end
function bayesian_score(vars, G, D)
n = length(vars)
M = statistics(vars, G, D)
α = prior(vars, G)
return sum(bayesian_score_component(M[i], α[i]) for i in 1:n)
end


Algorithm 5.2. K2 search of the space of directed acyclic graphs using a specified variable ordering. This variable ordering imposes a topological ordering in the resulting graph. The fit function takes an ordered list variables vars and a data set D. The method starts with an empty graph and iteratively adds the next parent that maximally improves the Bayesian score.

struct K2Search
ordering::Vector{Int} # variable ordering
end
function fit(method::K2Search, vars, D)
G = SimpleDiGraph(length(vars))
for (k,i) in enumerate(method.ordering[2:end])
y = bayesian_score(vars, G, D)
while true
y_best, j_best = -Inf, 0
for j in method.ordering[1:k]
if !has_edge(G, j, i)
add_edge!(G, j, i)
y′ = bayesian_score(vars, G, D)
if y′ > y_best
y_best, j_best = y′, j
end
rem_edge!(G, j, i)
end
end
if y_best > y
y = y_best
add_edge!(G, j_best, i)
else
break
end
end
end
return G
end


Algorithm 5.3. Local directed graph search, which starts with an initial directed graph G and opportunistically moves to a random graph neighbor whenever its Bayesian score is greater. It repeats this process for k_max iterations. Random graph neighbors are generated by either adding or removing a single edge. This algorithm can be extended to include reversing the direction of an edge. Edge addition can result in a graph with cycles, in which case we assign a score of  $ -\\infty


struct LocalDirectedGraphSearch
G # initial graph
k_max # number of iterations
end
function rand_graph_neighbor(G)
n = nv(G)
i = rand(1:n)
j = mod1(i + rand(2:n)-1, n)
G′ = copy(G)
has_edge(G, i, j) ? rem_edge!(G′, i, j) : add_edge!(G′, i, j)
return G′
end
function fit(method::LocalDirectedGraphSearch, vars, D)
G = method.G
y = bayesian_score(vars, G, D)
for k in 1:method.k_max
G′ = rand_graph_neighbor(G)
y′ = is_cyclic(G′) ? -Inf : bayesian_score(vars, G′, D)
if y′ > y
y, G = y′, G′
end
end
return G
end



Algorithm 5.4. A method for determining whether the directed acyclic graphs G and H are Markov equivalent. The subsets function from IterTools.jl returns all subsets of a given set and a specified size.

function are_markov_equivalent(G, H)
if nv(G) != nv(H) || ne(G) != ne(H) ||
!all(has_edge(H, e) || has_edge(H, reverse(e))
for e in edges(G))
return false
end
for (I, J) in [(G,H), (H,G)]
for c in 1:nv(I)
parents = inneighbors(I, c)
for (a, b) in subsets(parents, 2)
if !has_edge(I, a, b) && !has_edge(I, b, a) &&
!(has_edge(J, a, c) && has_edge(J, b, c))
return false
end
end
end
end
return true
end

```

They were translated to such:
---
**Algorithm 5.1.** An algorithm for computing the Bayesian score for a list of variables `variables` and a graph `graph` given data `data`. This method uses a uniform prior $ \alpha_{ijk} = 1 $ for all i, j, and k as generated by algorithm 4.2. The `loggamma` function is provided by `scipy.special`. Chapter 4 introduced the `statistics` and `prior` functions. Note that $ \log(\Gamma(\alpha)/\Gamma(\alpha+m)) = \log\Gamma(\alpha) - \log\Gamma(\alpha+m) $, and that $ \log\Gamma(1) = 0 $.
```python
def bayesian_score_component(M: np.ndarray, alpha: np.ndarray) -> float:
    # Note: The `loggamma` function is provided by `scipy.special`
    alpha_0 = np.sum(alpha, axis=1)
    p = np.sum(loggamma(alpha + M))
    p -= np.sum(loggamma(alpha))
    p += np.sum(loggamma(alpha_0))
    p -= np.sum(loggamma(alpha_0 + np.sum(M, axis=1)))
    return p
def bayesian_score(variables: list[Variable], graph: nx.DiGraph, data: np.ndarray) -> float:
    n = len(variables)
    M = statistics(variables, graph, data)
    alpha = prior(variables, graph)
    return np.sum([bayesian_score_component(M[i], alpha[i]) for i in range(n)])
```
---
**Algorithm 5.2.** K2 search of the space of directed acyclic graphs using a specified variable ordering. This variable ordering imposes a topological ordering in the resulting graph. The `fit` method takes an ordered list of variables `variables` and a data set `data`. The method starts with an empty graph and iteratively adds the next parent that maximally improves the Bayesian score.
```python
class K2Search(DirectedGraphSearchMethod):
    def __init__(self, ordering: list[int]):
        self.ordering = ordering
    def fit(self, variables: list[Variable], data: np.ndarray) -> nx.DiGraph:
        graph = nx.DiGraph()
        graph.add_nodes_from(range(len(variables)))
        y = bayesian_score(variables, graph, data)
        for k, i in enumerate(self.ordering[1:]):
            while True:
                y_best, j_best = -np.inf, 0
                for j in self.ordering[: k + 1]:
                    if not graph.has_edge(j, i):
                        graph.add_edge(j, i)
                        y_prime = bayesian_score(variables, graph, data)
                        if y_prime > y_best:
                            y_best, j_best = y_prime, j
                        graph.remove_edge(j, i)
                if y_best > y:
                    y = y_best
                    graph.add_edge(j_best, i)
                else:
                    break
        return graph
```
---
**Algorithm 5.3.** Local directed graph search, which starts with an initial directed graph `initial_graph` and opportunistically moves to a random graph neighbor whenever its Bayesian score is greater. It repeats this process for `k_max` iterations. Random graph neighbors are generated by either adding or removing a single edge. This algorithm can be extended to include reversing the direction of an edge. Edge addition can result in a graph with cycles, in which case we assign a score of $ -\infty $.
```python
class LocalDirectedGraphSearch(DirectedGraphSearchMethod):
    def __init__(self, initial_graph: nx.DiGraph, k_max: int):
        self.initial_graph = initial_graph
        self.k_max = k_max
    def fit(self, variables: list[Variable], data: np.ndarray) -> nx.DiGraph:
        graph = self.initial_graph
        y = bayesian_score(variables, graph, data)
        for k in range(self.k_max):
            graph_prime = self.rand_graph_neighbor(graph)
            if len(list(nx.simple_cycles(graph_prime))) == 0:
                y_prime = bayesian_score(variables, graph_prime, data)
            else:
                y_prime = -np.inf
            if y_prime > y:
                y, graph = y_prime, graph_prime
        return graph
    @staticmethod
    def rand_graph_neighbor(graph: nx.DiGraph) -> nx.DiGraph:
        n = graph.number_of_nodes()
        i = np.random.randint(low=0, high=n)
        j = (i + np.random.randint(low=1, high=n) - 1) % n
        graph_prime = graph.copy()
        if graph.has_edge(i, j):
            graph_prime.remove_edge(i, j)
        else:
            graph_prime.add_edge(i, j)
        return graph_prime
```
---
**Algorithm 5.4.** A method for determining whether the directed acyclic graphs `G` and `H` are Markov equivalent. The `combinations` function from `itertools` returns all subsets of a given set and a specified size.
```python
def are_markov_equivalent(G: nx.DiGraph, H: nx.DiGraph) -> bool:
    if ((G.number_of_nodes() != H.number_of_nodes()) or
        (G.number_of_edges() != H.number_of_edges()) or
        (not all([(H.has_edge(e[0], e[1]) or H.has_edge(e[1], e[0])) for e in G.edges]))):
        return False
    for (I, J) in [(G, H), (H, G)]:
        for c in range(I.number_of_nodes()):
            parents = list(I.predecessors(c))
            for a, b in itertools.combinations(parents, 2):
                if ((not I.has_edge(a, b)) and
                    (not I.has_edge(b, a)) and
                    (not (J.has_edge(a, c) and J.has_edge(b, c)))):
                    return False
    return True
```

Now I need to translate the fragments from the chapter 13:

{}
'''
